# ğŸ›ï¸ LLM Lawyer   End-to-End Legal Language Model (India)

A **from-scratch, domain-specialized Large Language Model** trained on Indian legal text, statutes, case law, and instruction-style question answering.

This repository implements the **complete Track-B LLM pipeline**   where every token, weight, and behavior originates from this codebase.  
No pretrained foundation models. No LoRA. No shortcut APIs in the learning loop.

Only **data â†’ tokens â†’ weights â†’ reasoning**.

---

## ğŸ§­ Project Scope

This project covers the **entire lifecycle of a GPT-style LLM**:

- Raw legal data collection (scrapers + PDFs)
- Unified corpus construction
- Custom BPE tokenizer training
- GPT pretraining from scratch
- Instruction & chat dataset generation
- Supervised Fine-Tuning (SFT)
- Hybrid inference (verified KB + generative LLM)
- CLI and REST API
- Quantization for deployment
- External evaluation via AI judge

Trained locally on **WSL + NVIDIA RTX 4070**.

---

## ğŸ—ºï¸ High-Level Pipeline

```

Scrapers / PDFs / Q&A
â†“
final_dataset.jsonl
â†“
Custom BPE Tokenizer
â†“
Binary Pretraining Data
â†“
GPT Pretraining (FlashAttention)
â†“
Instruction / Chat Dataset
â†“
Supervised Fine-Tuning (SFT)
â†“
Hybrid Inference (KB + LLM)
â†“
CLI / REST API / Quantized Model

```

Each stage produces **immutable artifacts** consumed by the next stage.  
Changing any upstream artifact invalidates everything downstream   intentionally.

---

## ğŸ“ Repository Structure

```

LLM_Lawyer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ base_pretrained/final_pretrained.pt
â”‚   â”‚   â””â”€â”€ sft/
â”‚   â”‚       â”œâ”€â”€ legal_llm_sft_final.pt
â”‚   â”‚       â””â”€â”€ legal_llm_quantized.pt
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”œâ”€â”€ pretrain/
â”‚   â”‚   â””â”€â”€ instruction/
â”‚   â”‚
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ model.py
â”‚       â”œâ”€â”€ hf_train_tokenizer.py
â”‚       â”œâ”€â”€ pack_pretraining_data.py
â”‚       â”œâ”€â”€ pretrain.py
â”‚       â”œâ”€â”€ pack_instruction_data.py
â”‚       â”œâ”€â”€ sft_train.py
â”‚       â”œâ”€â”€ infer_cli.py
â”‚       â”œâ”€â”€ api_server.py
â”‚       â””â”€â”€ quantize.py
â”‚â”€â”€frontend/
|   â”œâ”€â”€ index.html
|   â”œâ”€â”€ script.js
|   â””â”€â”€ README.md
â”œâ”€â”€ README.md
â”œâ”€â”€ setupguide.txt
â””â”€â”€ setuphardware.md


```

---

## ğŸ§  Data Collection & Corpus Construction

### Sources
- Indian Kanoon-style case law
- IPC, CrPC, Evidence Act, IT Act, Constitution
- Legal commentary and public-domain texts
- Synthetic legal Q&A (Gemini-assisted)
- Gutenberg books to learn english
- Grammar books to learn grammer, preposition, adjectives, verbs, etc.

All of this uploaded on Hugging Face here - https://huggingface.co/datasets/Meet-Jain-0170/LLM_Lawyer_Data
### Corpus Assembly
**Script:**  
`backend/src/pipeline/build_dataset.py`

All sources are cleaned, normalized, deduplicated, and merged into:

```

backend/data/processed/final_dataset.jsonl

````

Each line contains:
```json
{ "text": "Section 420 of the IPC deals with cheating..." }
````

This file is the **single source of truth** for tokenizer training and pretraining.

---

## ğŸ”¤ Tokenizer Training (BPE, From Scratch)

**Script:**
`backend/src/hf_train_tokenizer.py`

**Design**

* BPE tokenizer (`tokenizers`)
* Normalization: Unicode NFKC + lowercase
* Pre-tokenizer: whitespace
* Special tokens: `<PAD> <UNK> <BOS> <EOS>`
* BOS/EOS injected via post-processing template

**Output**

```
backend/data/tokenizer/legal_tokenizer.json
```

The tokenizer is the **root dependency** of the entire system.
Changing it invalidates all binary data and checkpoints.

---

## ğŸ§  GPT Pretraining (From Scratch)

### Binary Packing

**Script:**
`backend/src/pack_pretraining_data.py`

Produces:

```
backend/data/pretrain/
â”œâ”€â”€ train.bin
â”œâ”€â”€ val.bin
â””â”€â”€ meta.pkl
```

Flat, contiguous token streams optimized for high-throughput training.

---

### Pretraining

**Script:**
`backend/src/pretrain.py`

**Architecture**

* Decoder-only GPT
* 12 layers, 12 heads, d_model = 768
* Learned positional embeddings
* Pre-LayerNorm blocks
* FlashAttention v2
* Gradient checkpointing
* BF16 mixed precision
* Crash-safe checkpointing with resume

**Output**

```
backend/checkpoints/base_pretrained/final_pretrained.pt
```

This is the **foundation model**.
It learns legal language structure   not instructions.

---

## ğŸ§‘â€âš–ï¸ Instruction & Chat Dataset

### Instruction Format

**Script:**
`backend/src/prepare_chat_data.py`

Produces Alpaca-style and chat-style instruction datasets:

```
backend/data/processed/
â”œâ”€â”€ instruction_dataset.jsonl
â””â”€â”€ instruction_dataset_chat.jsonl
```

Chat format:

```
<|user|>
What is Section 420 IPC?
<|assistant|>
Section 420 IPC deals with cheating...
```

---

### Instruction Packing

**Script:**
`backend/src/pack_instruction_data.py`

Produces:

```
backend/data/instruction/
â”œâ”€â”€ train.bin
â”œâ”€â”€ labels.bin
â”œâ”€â”€ val.bin
â””â”€â”€ meta.pkl
```

Flat binary streams ensure deterministic SFT behavior.

---

## ğŸ¯ Supervised Fine-Tuning (SFT)

**Script:**
`backend/src/sft_train.py`

**Properties**

* Loads pretrained GPT checkpoint
* Architecture-locked compatibility
* Cosine LR decay with warmup
* Gradient accumulation
* Mixed precision (BF16 / FP16)
* Gradient clipping
* Periodic checkpointing

**Output**

```
backend/checkpoints/sft/
â”œâ”€â”€ ckpt_sft_*.pt
â””â”€â”€ legal_llm_sft_final.pt
```

SFT teaches **judgment and instruction following** on top of pretrained knowledge.

---

## ğŸ’¬ Hybrid Inference (CLI)

**Script:**
`backend/src/infer_cli.py`

Inference operates in **two phases**:

1. **Verified Knowledge Base**

   * Exact / keyword match from `legal_knowledge.py`
   * Deterministic, citation-safe answers

2. **Generative Fallback**

   * SFT-trained GPT
   * Low-temperature decoding
   * Context window safety
   * Output cleanup & artifact removal

Run:

```bash
python infer_cli.py
```

---

## ğŸŒ REST API Server

**Script:**
`backend/src/api_server.py`

**Endpoints**

* `GET /health`   model & device status
* `POST /chat`   hybrid legal inference

**Features**

* Thread-safe inference (CUDA lock)
* Same generation logic as CLI
* Frontend-ready JSON responses

---

## âš¡ Quantization

**Script:**
`backend/src/quantize.py`

* Dynamic INT8 quantization (Linear layers)
* CPU-friendly deployment
* Reduced memory footprint

**Output**

```
backend/checkpoints/sft/legal_llm_quantized.pt
```

---

## ğŸ“Š Evaluation

**Script:**
`backend/src/evaluation.py`

* Uses an external LLM as an **impartial judge**
* Compares model answers to expert answers
* Returns structured JSON scores and explanations

This is **evaluation only**   never used in training.

---

## ğŸ§  Model Architecture (Single Source of Truth)

**File:**
`backend/src/model.py`

Defines:

* `GPTConfig`
* Decoder-only GPT
* FlashAttention-compatible attention
* Checkpoint-stable parameter naming

Any change here invalidates **all checkpoints**.

---

## ğŸ” Reproducibility Contract

Given:

* `final_dataset.jsonl`
* `legal_tokenizer.json`
* Training scripts and configs

This pipeline deterministically reproduces:

* Token boundaries
* Binary datasets
* Model architecture
* Checkpoint compatibility
* Inference behavior

---

## ğŸš€ Future Work

* Larger conversational SFT corpus
* Preference optimization (DPO / RLHF-lite)
* Citation-grounded RAG
* Multilingual Indian law
* Dockerized GPU deployment

---

## ğŸ Summary

**LLM Lawyer** is a complete, end-to-end implementation of a **Track-B Large Language Model**, built the hard way   from raw legal text to a deployable reasoning system.

Nothing here is inherited.
Nothing is abstracted away.
Nothing learns outside this repository.

The project demonstrates:

- How a **custom tokenizer** defines the linguistic universe of a model  
- How **binary-packed corpora** enable stable, high-throughput GPT training  
- How **from-scratch pretraining** builds domain intuition before instruction bias  
- How **supervised fine-tuning** teaches judgment without erasing knowledge  
- How **hybrid inference** balances deterministic law with probabilistic reasoning  

This is not a chatbot pretending to know the law.  
It is a system that **learned legal language step by step**, under explicit constraints.

---

## âš–ï¸ Intended Use & Disclaimer

This project is:

- A research and learning artifact
- A demonstration of LLM systems engineering
- A legal-domain modeling experiment

It is **not**:

- A substitute for professional legal advice
- A certified legal expert system
- A production-hardened service

Always verify legal conclusions with qualified professionals.

---

## ğŸ§‘â€ğŸ’» Author

**Meet Jain**  - Tokenizer, SFT, Quantization, Evaluation.

**Anshul Roy**  - Model building, Pre-training.

**Kevin Patel** = Data Collection, Preprocessing.

**Cleon D'Souza** = API Server building, Frontend UI/UX.

**Atlas SkillTech University** - TY B.Tech (AI / ML)  

---

## ğŸ“Œ Closing Note

Large Language Models are not trained.  
They are **constructed**.

This repository documents that construction    
cleanly, explicitly, and without illusion.

If you understand this codebase,  
you donâ€™t just *use* LLMs.

You **own the process**.

